import time
import httpx
from typing import Optional, Dict, Any, Tuple, List
from urllib.parse import quote, unquote
import re

# cache: qid -> (timestamp, data)
_CACHE: Dict[str, Tuple[float, Dict[str, Any]]] = {}
TTL_SECONDS = 60 * 30  # 30 minutes

WIKIDATA_HEADERS = {
    "User-Agent": "IoanninaExplorer/1.0 (University project; contact: filip.chatziergatis@gmail.com)",
    "Accept": "application/json",
}
async def fetch_wikipedia_short_description(wikipedia_url: str) -> Optional[str]:
    if not wikipedia_url:
        return None

    lang = "el" if "el.wikipedia.org" in wikipedia_url else "en"
    title = wikipedia_url.split("/wiki/")[-1]
    title = unquote(title)

    url = f"https://{lang}.wikipedia.org/api/rest_v1/page/summary/{title}"

    async with httpx.AsyncClient(headers=WIKIDATA_HEADERS, timeout=10.0) as client:
        r = await client.get(url)
        if r.status_code != 200:
            return None

        data = r.json()
        extract = data.get("extract")
        if not extract:
            return None

        # ğŸ‘‰ ÎšÏÎ¬Ï„Î± Î¼ÏŒÎ½Î¿ Ï„Î¹Ï‚ Ï€ÏÏÏ„ÎµÏ‚ 2â€“3 Ï€ÏÎ¿Ï„Î¬ÏƒÎµÎ¹Ï‚
        sentences = re.split(r'(?<=[.!;])\s+', extract)
        return " ".join(sentences[:3])

def _cache_get(key: str) -> Optional[Dict[str, Any]]:
    item = _CACHE.get(key)
    if not item:
        return None
    ts, data = item
    if time.time() - ts > TTL_SECONDS:
        _CACHE.pop(key, None)
        return None
    return data


def _cache_set(key: str, data: Dict[str, Any]) -> None:
    _CACHE[key] = (time.time(), data)


def commons_file_url(filename: str, width: int = 1100) -> str:
    # âœ… ÏƒÏ‰ÏƒÏ„ÏŒ URL Î±Ï€ÏŒ Wikimedia Commons
    return f"https://commons.wikimedia.org/wiki/Special:FilePath/{quote(filename)}?width={width}"


async def fetch_wikidata_entity(qid: str) -> Optional[Dict[str, Any]]:
    """
    Reliable endpoint (Ï‡Ï‰ÏÎ¯Ï‚ wbgetentities): Special:EntityData
    """
    cache_key = f"wd:{qid}"
    cached = _cache_get(cache_key)
    if cached:
        return cached

    url = f"https://www.wikidata.org/wiki/Special:EntityData/{qid}.json"
    try:
        async with httpx.AsyncClient(
                headers=WIKIDATA_HEADERS,
                timeout=20.0,
                follow_redirects=True
        ) as client:
            r = await client.get(url)
            if r.status_code != 200:
                print(f"âš ï¸ Wikidata returned {r.status_code} for {qid}")
                return None

            data = r.json()
            _cache_set(cache_key, data)
            return data
    except Exception as e:
        print(f"âš ï¸ Wikidata request failed for {qid}: {e}")
        return None


# ------------------ Wikidata parsing helpers ------------------

def _get_entity(data: Dict[str, Any], qid: str) -> Dict[str, Any]:
    return (data.get("entities") or {}).get(qid) or {}


def _get_claims(entity: Dict[str, Any], prop: str) -> List[Dict[str, Any]]:
    return (entity.get("claims") or {}).get(prop) or []


def _get_claim_str(entity: Dict[str, Any], prop: str) -> Optional[str]:
    claims = _get_claims(entity, prop)
    if not claims:
        return None
    dv = claims[0].get("mainsnak", {}).get("datavalue", {})
    v = dv.get("value")
    return v if isinstance(v, str) else None


def _get_claim_url(entity: Dict[str, Any], prop: str) -> Optional[str]:
    return _get_claim_str(entity, prop)


def _get_claim_quantity(entity: Dict[str, Any], prop: str) -> Optional[float]:
    claims = _get_claims(entity, prop)
    if not claims:
        return None
    v = claims[0].get("mainsnak", {}).get("datavalue", {}).get("value", {})
    amount = v.get("amount")
    if not isinstance(amount, str):
        return None
    try:
        return float(amount.replace("+", ""))
    except:
        return None


def _get_claim_time_year(entity: Dict[str, Any], prop: str) -> Optional[str]:
    claims = _get_claims(entity, prop)
    if not claims:
        return None
    v = claims[0].get("mainsnak", {}).get("datavalue", {}).get("value", {})
    t = v.get("time")
    if not isinstance(t, str) or len(t) < 5:
        return None
    # "+1880-00-00T00:00:00Z" -> "1880"
    return t[1:5]


def _get_claim_entity_id(entity: Dict[str, Any], prop: str) -> Optional[str]:
    claims = _get_claims(entity, prop)
    if not claims:
        return None
    v = claims[0].get("mainsnak", {}).get("datavalue", {}).get("value", {})
    # {"id":"Qxxx"...}
    if isinstance(v, dict):
        return v.get("id")
    return None


def _get_coordinates(entity: Dict[str, Any]) -> Tuple[Optional[float], Optional[float]]:
    claims = _get_claims(entity, "P625")
    if not claims:
        return None, None
    v = claims[0].get("mainsnak", {}).get("datavalue", {}).get("value", {})
    if not isinstance(v, dict):
        return None, None
    return v.get("latitude"), v.get("longitude")


def _get_wikipedia_link(sitelinks: Dict[str, Any], lang_pref=("elwiki", "enwiki")) -> Optional[str]:
    for key in lang_pref:
        sl = sitelinks.get(key)
        if sl and "title" in sl:
            title = sl["title"].replace(" ", "_")
            lang = "el" if key == "elwiki" else "en"
            return f"https://{lang}.wikipedia.org/wiki/{title}"
    return None


def parse_poi_from_wikidata(qid: str, data: Dict[str, Any]) -> Dict[str, Any]:
    """
    Î•Ï€Î¹ÏƒÏ„ÏÎ­Ï†ÎµÎ¹:
    - title/description/coords/image/wikipediaUrl
    - facts: Ï‰ÏÎ±Î¯Î± â€œÎ­Î¾Ï„ÏÎ±â€ Ï€Î¿Ï… Î¼Ï€Î¿ÏÎµÎ¯Ï‚ Î½Î± Î´ÎµÎ¯Î¾ÎµÎ¹Ï‚ ÏƒÏ„Î¿ 'Î ÎµÏÎ¹ÏƒÏƒÏŒÏ„ÎµÏÎ±'
    - raw: ÎŸÎ›ÎŸ Ï„Î¿ Wikidata entity (claims/labels/sitelinks ÎºÏ„Î»)
    """
    entity = _get_entity(data, qid)

    labels = entity.get("labels", {}) or {}
    descriptions = entity.get("descriptions", {}) or {}
    sitelinks = entity.get("sitelinks", {}) or {}

    title = (labels.get("el") or labels.get("en") or {}).get("value")
    description = (descriptions.get("el") or descriptions.get("en") or {}).get("value")

    lat, lon = _get_coordinates(entity)

    photoname = _get_claim_str(entity, "P18")  # image filename
    image_url = commons_file_url(photoname, 1200) if photoname else None

    wikipedia_url = _get_wikipedia_link(sitelinks)

    # ===== EXTRA FACTS (ÏŒ,Ï„Î¹ Ï…Ï€Î¬ÏÏ‡ÎµÎ¹) =====
    facts: List[Dict[str, str]] = []

    # Official website
    website = _get_claim_url(entity, "P856")
    if website:
        facts.append({"label": "Î™ÏƒÏ„ÏŒÏ„Î¿Ï€Î¿Ï‚", "value": website})

    # Elevation
    elevation = _get_claim_quantity(entity, "P2044")
    if elevation is not None:
        facts.append({"label": "Î¥ÏˆÏŒÎ¼ÎµÏ„ÏÎ¿", "value": f"{int(round(elevation))} m"})

    # Inception year
    year = _get_claim_time_year(entity, "P571")
    if year:
        facts.append({"label": "ÎˆÏ„Î¿Ï‚", "value": year})

    # Commons category
    commons_category = _get_claim_str(entity, "P373")
    if commons_category:
        facts.append({"label": "Commons category", "value": commons_category})

    # Located in (admin entity)
    located_in_q = _get_claim_entity_id(entity, "P131")
    if located_in_q:
        facts.append({"label": "Î¤Î¿Ï€Î¿Î¸ÎµÏƒÎ¯Î± (Wikidata)", "value": f"https://www.wikidata.org/wiki/{located_in_q}"})

    # Instance of
    instance_of_q = _get_claim_entity_id(entity, "P31")
    if instance_of_q:
        facts.append({"label": "Î¤ÏÏ€Î¿Ï‚ (Wikidata)", "value": f"https://www.wikidata.org/wiki/{instance_of_q}"})

    # Country
    country_q = _get_claim_entity_id(entity, "P17")
    if country_q:
        facts.append({"label": "Î§ÏÏÎ± (Wikidata)", "value": f"https://www.wikidata.org/wiki/{country_q}"})

    return {
        "title": title,
        "description": description,
        "lat": lat,
        "lon": lon,
        "image": image_url,
        "wikipediaUrl": wikipedia_url,
        "facts": facts,
        "raw": entity,  # âœ… ÎŸÎ›ÎŸ Ï„Î¿ entity
    }
